{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "internship.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5MMSu8cy41B",
        "outputId": "a6671398-d334-441f-c488-8290263daac7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "#drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK0gZnQcyEf7",
        "outputId": "482cd5a0-da90-4f35-bbb1-870525572d55"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import os \n",
        "import numpy as np\n",
        "state = '/content/gdrive/MyDrive/Dataset/'\n",
        "# file_name = '002.csv' \n",
        "# mathur = pd.read_csv(state + file_name , sep='delimiter', engine='python')\n",
        "# print(mathur)\n",
        "arr_x = []\n",
        "arr_y = []\n",
        "names = set()\n",
        "j = 0\n",
        "for i in os.listdir(state):\n",
        "#  print(i)\n",
        "  with open(state+i) as csv_file:\n",
        "    mathur = csv.reader(csv_file,delimiter = ',')\n",
        "    temp = []\n",
        "    for (j,row) in enumerate(mathur):\n",
        "      if j == 0 :\n",
        "        st = row[0][8:]\n",
        "#        print(st)\n",
        "        arr_y.append(st)\n",
        "        names.add(st)\n",
        "      if j < 12 :\n",
        "        continue\n",
        "      if j > 630 :\n",
        "        break\n",
        "      if len(row) < 2 :\n",
        "        continue     \n",
        "#      print(row)\n",
        "      temp_arr = []\n",
        "      temp_arr.append(float(row[0]))\n",
        "      temp_arr.append(float(row[1]))\n",
        "      temp.append(temp_arr)\n",
        "# print(len(arr_x))\n",
        "    arr_x.append(temp)\n",
        "#    arr_y.append(i)\n",
        "min_val = 100000\n",
        "for i in arr_x :\n",
        "  min_val = min(min_val, len(i))\n",
        " \n",
        "dic = {}\n",
        "for (i,val) in enumerate(names):\n",
        "    dic[val] = i\n",
        "#print(d)\n",
        "#print(d[stt])\n",
        "arr_yy = []\n",
        "for i in arr_y:\n",
        "    arr_yy.append(dic[i])\n",
        "arr_y = arr_yy    \n",
        "#print(min_val)\n",
        "#print(arr_x[0]) \n",
        "print(arr_y)\n",
        "#print(len(dic))\n",
        "print(len(arr_y))\n",
        "print(len(arr_x[0]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 5, 5, 5, 5, 5, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0]\n",
            "19\n",
            "451\n",
            "619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBrDNc4zrtyO",
        "outputId": "ca64c7c0-24c2-47c7-c106-df36e37908a8"
      },
      "source": [
        "yyyy = np.zeros((451,len(dic)),'float32')\n",
        "for (i,j) in enumerate(arr_y):\n",
        "#  print(i,j)\n",
        "  yyyy[i][j-1] = 1\n",
        "\n",
        "print(yyyy.shape)\n",
        "# print(min_val)\n",
        "print(len(arr_x))\n",
        "# arr_xx = np.array(arr_x)\n",
        "# print(arr_xx.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "splittt = train_test_split(yyyy, arr_x, test_size=0.1, random_state=42)\n",
        "(a, b, c, d) = splittt\n",
        "split = train_test_split(a , c, test_size = 0.3, random_state=42)\n",
        "(trainAttrX, testAttrX, trainSpectraX, testSpectraX) = split\n",
        "print(len(trainAttrX))\n",
        "print(len(a))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(451, 19)\n",
            "451\n",
            "283\n",
            "405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JezvVLzq5ts5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "outputId": "8957f0ff-8f46-466d-aea2-844d6b48a771"
      },
      "source": [
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, concatenate, merge\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input,GlobalAveragePooling1D,Flatten\n",
        "from keras.layers.convolutional import Convolution1D,MaxPooling1D, Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "#dataset = loadtxt('/content/gdrive/MyDrive/Dataset/pima-indians-diabetes.csv', delimiter=',')\n",
        "#print(dataset)\n",
        "#X = dataset[:,0:8]\n",
        "#y = dataset[:,8]\n",
        "ax = np.asarray(trainSpectraX).astype('float32')\n",
        "y = np.asarray(trainAttrX).astype('float32')\n",
        "x = ax.reshape(len(trainSpectraX),619,2,1)\n",
        "testSX = np.asarray(testSpectraX).astype('float32')\n",
        "tAX = np.asarray(testAttrX).astype('float32')\n",
        "tSX = testSX.reshape(len(testSX),619,2,1)\n",
        "#tAX = testAX.reshape(len(testAX),619,2,1)\n",
        "# print(x)\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "model1= Sequential()\n",
        "model2= Sequential()\n",
        "model3= Sequential()\n",
        "model4= Sequential()\n",
        "input_sh = (619,2,1)\n",
        "\n",
        "model1.add(Convolution1D(filters=16, kernel_size=21, padding='same', activation='LeakyReLU', input_shape=input_sh))\n",
        "model1.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model1.add(BatchNormalization())\n",
        "model1.summary()\n",
        "\n",
        "model2.add(Convolution1D(filters=32, kernel_size=11, padding='same', activation='LeakyReLU', input_shape= input_sh))\n",
        "model2.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.summary()\n",
        "\n",
        "model3.add(Convolution1D(filters=64, kernel_size=5, padding='same', activation='LeakyReLU', input_shape= input_sh))\n",
        "model3.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.summary()\n",
        "\n",
        "model4 = concatenate([model1.output, model2.output, model3.output], axis= -1)\n",
        "\n",
        "model4.add(Flatten())\n",
        "model4.add(Dense(2048, activation='tanh'))\n",
        "model4.add(Dropout(.5))\n",
        "model4.add(Dense(len(dic), activation=\"softmax\"))\n",
        "model4.summary()\n",
        "\n",
        "model4.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])\n",
        "model4.fit(x, y, validation_data = (tSX, tAX), epochs=50, batch_size=10, verbose=2)\n",
        "# # _, accuracy = model.evaluate(x, y)\n",
        "# print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(283, 619, 2, 1)\n",
            "(283, 19)\n",
            "Model: \"sequential_59\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_45 (Conv1D)           (None, 619, 2, 16)        352       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 310, 1, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 310, 1, 16)        64        \n",
            "=================================================================\n",
            "Total params: 416\n",
            "Trainable params: 384\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_60\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_46 (Conv1D)           (None, 619, 2, 32)        384       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 310, 1, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 310, 1, 32)        128       \n",
            "=================================================================\n",
            "Total params: 512\n",
            "Trainable params: 448\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_61\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_47 (Conv1D)           (None, 619, 2, 64)        384       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 310, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 310, 1, 64)        256       \n",
            "=================================================================\n",
            "Total params: 640\n",
            "Trainable params: 512\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-bf7ad914aa4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mmodel4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KerasTensor' object has no attribute 'add'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWEPL6oSwE_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fccb16-1c70-4c46-d174-17e07ae3fb92"
      },
      "source": [
        "bb = np.asarray(b).astype('float32')\n",
        "dd = np.asarray(d).astype('float32')\n",
        "ddd= dd.reshape(len(dd),619,2,1)\n",
        "\n",
        "scores = model.evaluate(ddd, bb, verbose=0)\n",
        "print(scores)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.5639052391052246, 0.8220859169960022]\n",
            "accuracy: 82.21%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEp3OQApg9rt",
        "outputId": "bb6f9be4-bf07-4b23-cee4-b6d04c06eb7b"
      },
      "source": [
        "with open('/content/gdrive/MyDrive/Xnew/'+ 'xnew.txt') as csv_file:\n",
        "    m = csv.reader(csv_file,delimiter = ',')\n",
        "    xxnew = []\n",
        "    for (j,row) in enumerate(m):\n",
        "      if j == 0 :\n",
        "        stt = row[0][8:]\n",
        "      if j < 12 :\n",
        "        continue\n",
        "      if j > 630 :\n",
        "        break\n",
        "      if len(row) < 2 :\n",
        "        continue     \n",
        "      xx = []\n",
        "      xx.append(float(row[0]))\n",
        "      xx.append(float(row[1]))\n",
        "      xxnew.append(xx)\n",
        "#    print(xxnew)\n",
        "axnew = np.asarray(xxnew).astype('float32')\n",
        "xnew = axnew.reshape(1,619,2,1)\n",
        "print(stt)\n",
        "print(dic.get(stt))\n",
        "print(dic)\n",
        "ans = model.predict_classes(xnew)\n",
        "print(ans)\n",
        "# for i in d:\n",
        "#   if(d[i] == ans):\n",
        "#     print(i)\n",
        "# print(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuprite\n",
            "48\n",
            "{'Chabazite-Ca': 0, 'Laverovite': 1, 'Chenevixite': 2, 'Scheelite': 3, 'Kazanskyite': 4, 'Augelite': 5, 'Epididymite': 6, 'Pyrite': 7, 'Partheite': 8, 'Kutnohorite': 9, 'Staurolite': 10, 'Monticellite': 11, 'Inderite': 12, 'Bertrandite': 13, 'Ilvaite': 14, 'Columbite-(Fe)': 15, 'Davidlloydite': 16, 'Lazulite': 17, 'Dioptase': 18, 'Magnesiotaaffeite-6N3S': 19, 'Euclase': 20, 'Atacamite': 21, 'Colemanite': 22, 'Sinoite': 23, 'Triplite': 24, 'Quartz': 25, 'Ludlamite': 26, 'Chromatite': 27, 'Montebrasite': 28, 'Tilleyite': 29, 'Inyoite': 30, 'Leucophanite': 31, 'Bikitaite': 32, 'Corundum': 33, 'Rhodochrosite': 34, 'Hughesite': 35, 'Beryl': 36, 'Calcite': 37, 'Elbaite': 38, 'Sodalite': 39, 'Laumontite': 40, 'Painite': 41, 'Galuskinite': 42, 'Fluoro-richterite': 43, 'Witherite': 44, 'Bobdownsite': 45, 'Brookite': 46, 'Sulphur': 47, 'Cuprite': 48, 'Gypsum': 49, 'Forsterite': 50, 'Pyrosmalite-(Fe)': 51, 'Hardystonite': 52, 'Danburite': 53, 'Edenite': 54, 'Fluorapophyllite-(K)': 55, 'Mesolite': 56, 'Smithsonite': 57, 'Volaschioite': 58, 'Olivenite': 59, 'Creedite': 60, 'Diopside': 61, 'Silicon': 62, 'Clinochlore': 63, 'Muscovite': 64, 'Braunite': 65, 'Axinite-(Fe)': 66, 'Zircon': 67, 'Erionite-Ca': 68, 'Microcline': 69, 'Gehlenite': 70, 'Cerussite': 71, 'Eudialyte': 72, 'Pharmacosiderite': 73, 'Monazite-(Ce)': 74, 'Diaspore': 75, 'Kyanite': 76, 'Anorthite': 77, 'Metatorbernite': 78, 'Tremolite': 79, 'Pyromorphite': 80, 'Strontiohurlbutite': 81, 'Brazilianite': 82, 'Boleite': 83, 'Clinohumite': 84, 'Diamond': 85, 'Gmelinite-Na': 86, 'Nepheline': 87, 'Riebeckite': 88, 'Phillipsite-Ca': 89, 'Vanadinite': 90, 'Whitlockite': 91, 'Cordierite': 92, 'Sanbornite': 93, 'Topaz': 94, 'Hausmannite': 95, 'Jadeite': 96, 'Weloganite': 97, 'Spodumene': 98, 'Magnesio-hastingsite': 99, 'Datolite': 100, 'Grunerite': 101, 'Kanoite': 102, 'Chukhrovite-(Ca)': 103, 'Vivianite': 104, 'Rhodonite': 105, 'Marialite': 106, 'Scholzite': 107, 'Fluorophlogopite': 108, 'Hemimorphite': 109, 'Clinoptilolite-Ca': 110, 'Atelisite-(Y)': 111, 'Barysilite': 112, 'Chrysoberyl': 113, 'Gahnite': 114, 'Carrollite': 115, 'Arfvedsonite': 116, 'Pyrope': 117, 'Goethite': 118, 'Astrophyllite': 119, 'Hubnerite': 120, 'Arsenopyrite': 121, 'Fluorapatite': 122, 'Ferro-actinolite': 123, 'Baryte': 124, 'Fluor-uvite': 125, 'Zoisite': 126, 'Enstatite': 127, 'Epidote': 128, 'Lecoqite-(Y)': 129, 'Edingtonite': 130, 'Chlormayenite': 131, 'Augite': 132, 'Plumbophyllite': 133, 'Cassiterite': 134, 'Grossular': 135, 'Aragonite': 136, 'Senegalite': 137, 'Whewellite': 138, 'Wavellite': 139, 'Chondrodite': 140, 'Kornerupine': 141, 'Barrerite': 142, 'Crocoite': 143, 'Natrolite': 144, 'Pectolite': 145, 'Ferberite': 146, 'Stilbite-Ca': 147, 'Hauyne': 148, 'Chalcopyrite': 149, 'Cancrinite': 150, 'Clinoenstatite': 151, 'Thomsonite-Ca': 152, 'Cubic zirconia': 153, 'Shortite': 154, 'Celestine': 155, 'Meionite': 156, 'Lawsonite': 157, 'Fluor-buergerite': 158, 'Hedenbergite': 159, 'Talc': 160, 'Dravite': 161, 'Azurite': 162, 'Catapleiite': 163, 'Phenakite': 164, 'Clinozoisite': 165, 'Siderite': 166, 'Bustamite': 167, 'Olmiite': 168, 'Humite': 169, 'Vayrynenite': 170, 'Polylithionite': 171, 'Titanite': 172, 'Chloritoid': 173, 'Strontianite': 174, 'Covellite': 175, 'Serandite': 176, 'Fluorite': 177, 'Scolecite': 178, 'Mimetite': 179, 'Paravauxite': 180, 'Gaspeite': 181, 'Trilithionite': 182, 'Rutile': 183, 'Dolomite': 184, 'Vesuvianite': 185, 'Baddeleyite': 186, 'Wulfenite': 187, 'Fayalite': 188, 'Kurnakovite': 189, 'Xenotime-(Y)': 190, 'Magnesite': 191, 'Heulandite-Ca': 192, 'Huntite': 193, 'Rhodizite': 194, 'Sphalerite': 195, 'Erythrite': 196, 'Hydroxylbastnasite-(Ce)': 197, 'Orthoclase': 198, 'Legrandite': 199, 'Spessartine': 200, 'Stellerite': 201, 'Gaylussite': 202, 'Brucite': 203}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[47]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3xTyP9jF5ev"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}